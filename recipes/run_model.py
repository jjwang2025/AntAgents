#!/usr/bin/env python3
"""
å¤§æ¨¡å‹è°ƒç”¨ç¤ºä¾‹ - run_model.py

åŸºäº OpenAI å…¼å®¹ API çš„å¤§æ¨¡å‹è°ƒç”¨å®ç°ï¼Œæ”¯æŒæµå¼å’Œéæµå¼è°ƒç”¨ã€‚
"""

import os
import sys
from typing import List, Optional, Generator, Dict, Any
from enum import Enum
import json
from dotenv import load_dotenv

from antagents import (
    OpenAIServerModel,
    ChatMessage,
    MessageRole,
    ChatMessageStreamDelta,
    TokenUsage
)


def create_example_messages() -> List[ChatMessage]:
    """åˆ›å»ºç¤ºä¾‹æ¶ˆæ¯"""
    return [
        ChatMessage(
            role=MessageRole.SYSTEM,
            content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œå›ç­”è¦ç®€æ´æ˜äº†ã€‚"
        ),
        ChatMessage(
            role=MessageRole.USER,
            content="è¯·è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µå’Œåº”ç”¨é¢†åŸŸã€‚"
        )
    ]


def print_messages_detailed(messages: List[ChatMessage]):
    """ç¾è§‚åœ°æ‰“å°æ¶ˆæ¯è¯¦æƒ…"""
    print("ğŸ” è¾“å…¥çš„Promptè¯¦æƒ…:")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
    
    for i, msg in enumerate(messages, 1):
        # è§’è‰²å›¾æ ‡æ˜ å°„
        role_icons = {
            MessageRole.SYSTEM: "âš™ï¸",
            MessageRole.USER: "ğŸ‘¤", 
            MessageRole.ASSISTANT: "ğŸ¤–",
            MessageRole.TOOL_CALL: "ğŸ› ï¸",
            MessageRole.TOOL_RESPONSE: "ğŸ“‹"
        }
        
        role_emoji = role_icons.get(msg.role, "ğŸ“")
        
        print(f"{role_emoji} æ¶ˆæ¯ {i} [{msg.role.value.upper()}]:")
        
        # å¤„ç†å†…å®¹ï¼ˆæ”¯æŒå¤šæ¨¡æ€ï¼‰
        if isinstance(msg.content, list):
            print(f"   ğŸ“„ å†…å®¹ç±»å‹: å¤šæ¨¡æ€ ({len(msg.content)} ä¸ªéƒ¨åˆ†)")
            for j, part in enumerate(msg.content, 1):
                if isinstance(part, dict):
                    if part.get('type') == 'text':
                        content_preview = part.get('text', '')[:100] + '...' if len(part.get('text', '')) > 100 else part.get('text', '')
                        print(f"      {j}. æ–‡æœ¬: {content_preview}")
                    elif part.get('type') in ['image', 'image_url']:
                        print(f"      {j}. å›¾åƒ: [å›¾åƒå†…å®¹]")
        else:
            content_preview = str(msg.content)[:100] + '...' if msg.content and len(str(msg.content)) > 100 else msg.content
            print(f"   ğŸ“„ å†…å®¹: {content_preview}")
        
        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨
        if msg.tool_calls:
            print(f"   ğŸ› ï¸  å·¥å…·è°ƒç”¨: {len(msg.tool_calls)} ä¸ª")
            for tool_call in msg.tool_calls:
                if hasattr(tool_call, 'function'):
                    print(f"      - {tool_call.function.name}")
        
        # æ˜¾ç¤ºtokenä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœæœ‰ï¼‰
        if msg.token_usage:
            print(f"   ğŸ“Š Tokenä½¿ç”¨: è¾“å…¥={msg.token_usage.input_tokens}, è¾“å‡º={msg.token_usage.output_tokens}")
        
        if i < len(messages):
            print("   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")


def non_streaming_example(model: OpenAIServerModel, messages: List[ChatMessage]):
    """éæµå¼è°ƒç”¨ç¤ºä¾‹"""
    print("\nğŸš€ å¼€å§‹éæµå¼è°ƒç”¨...")
    print("=" * 60)
    
    try:
        response = model.generate(messages=messages)
        
        print("âœ… éæµå¼è°ƒç”¨æˆåŠŸï¼")
        print("\nğŸ“‹ æ¨¡å‹å›å¤:")
        print("-" * 40)
        print(response.content)
        print("-" * 40)
        
        if response.token_usage:
            print("\nğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡:")
            print(f"   ğŸ“¥ è¾“å…¥Token: {response.token_usage.input_tokens}")
            print(f"   ğŸ“¤ è¾“å‡ºToken: {response.token_usage.output_tokens}")
            print(f"   ğŸ“Š æ€»Token: {response.token_usage.input_tokens + response.token_usage.output_tokens}")
        
        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
        if response.tool_calls:
            print("\nğŸ› ï¸  å·¥å…·è°ƒç”¨:")
            for tool_call in response.tool_calls:
                if hasattr(tool_call, 'function'):
                    print(f"   - {tool_call.function.name}: {tool_call.function.arguments}")
            
    except Exception as e:
        print(f"âŒ éæµå¼è°ƒç”¨è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


def streaming_example(model: OpenAIServerModel, messages: List[ChatMessage]):
    """æµå¼è°ƒç”¨ç¤ºä¾‹"""
    print("\nğŸš€ å¼€å§‹æµå¼è°ƒç”¨...")
    print("=" * 60)
    
    try:
        print("ğŸ“ æ¨¡å‹å›å¤ (æµå¼):")
        print("-" * 40)
        
        full_response = ""
        final_token_usage = TokenUsage(input_tokens=0, output_tokens=0)
        tool_calls_accumulated = []
        
        for delta in model.generate_stream(messages=messages):
            if delta.content:
                print(delta.content, end="", flush=True)
                full_response += delta.content
            
            if delta.token_usage:
                final_token_usage.input_tokens += delta.token_usage.input_tokens
                final_token_usage.output_tokens += delta.token_usage.output_tokens
            
            # å¤„ç†å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
            if delta.tool_calls:
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ›´å¤æ‚çš„ç´¯ç§¯é€»è¾‘
                tool_calls_accumulated.extend(delta.tool_calls)
        
        print("\n" + "-" * 40)
        
        print("\nğŸ“Š Token ä½¿ç”¨ç»Ÿè®¡:")
        print(f"   ğŸ“¥ è¾“å…¥Token: {final_token_usage.input_tokens}")
        print(f"   ğŸ“¤ è¾“å‡ºToken: {final_token_usage.output_tokens}")
        print(f"   ğŸ“Š æ€»Token: {final_token_usage.input_tokens + final_token_usage.output_tokens}")
        
        # æ˜¾ç¤ºå·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
        if tool_calls_accumulated:
            print("\nğŸ› ï¸  å·¥å…·è°ƒç”¨æ£€æµ‹åˆ°ï¼ˆæµå¼æ¨¡å¼ä¸‹å·¥å…·è°ƒç”¨éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰")
            
    except Exception as e:
        print(f"âŒ æµå¼è°ƒç”¨è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


def interactive_chat(model: OpenAIServerModel):
    """äº¤äº’å¼èŠå¤©ç¤ºä¾‹"""
    print("\nğŸ’¬ è¿›å…¥äº¤äº’å¼èŠå¤©æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
    print("=" * 60)
    
    # ç³»ç»Ÿæç¤ºè¯
    system_message = ChatMessage(
        role=MessageRole.SYSTEM,
        content="ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡è¿›è¡Œå‹å¥½ã€ä¸“ä¸šçš„å¯¹è¯ã€‚"
    )
    
    messages = [system_message]
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ æ‚¨: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ é€€å‡ºäº¤äº’æ¨¡å¼")
                break
                
            if not user_input:
                continue
                
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            user_message = ChatMessage(role=MessageRole.USER, content=user_input)
            messages.append(user_message)
            
            print("\nğŸ¤– AI: ", end="", flush=True)
            
            # æµå¼å“åº”
            response_content = ""
            token_usage = TokenUsage(input_tokens=0, output_tokens=0)
            
            for delta in model.generate_stream(messages=messages):
                if delta.content:
                    print(delta.content, end="", flush=True)
                    response_content += delta.content
                
                if delta.token_usage:
                    token_usage.input_tokens += delta.token_usage.input_tokens
                    token_usage.output_tokens += delta.token_usage.output_tokens
            
            # æ·»åŠ AIå›å¤åˆ°æ¶ˆæ¯å†å²
            if response_content:
                assistant_message = ChatMessage(
                    role=MessageRole.ASSISTANT, 
                    content=response_content,
                    token_usage=token_usage
                )
                messages.append(assistant_message)
                
            # æ˜¾ç¤ºæœ¬æ¬¡è°ƒç”¨çš„tokenä½¿ç”¨
            print(f"\n   ğŸ“Š æœ¬æ¬¡è°ƒç”¨Token: è¾“å…¥={token_usage.input_tokens}, è¾“å‡º={token_usage.output_tokens}")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºäº¤äº’æ¨¡å¼")
            break
        except Exception as e:
            print(f"\nâŒ èŠå¤©è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            break


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤– å¤§æ¨¡å‹è°ƒç”¨ç¤ºä¾‹ç¨‹åº")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ¨¡å‹
    try:
        model = OpenAIServerModel(
            model_id=os.getenv("DEEPSEEK_MODEL_ID"),
            api_base=os.getenv("DEEPSEEK_URL"),
            api_key=os.getenv("DEEPSEEK_API_KEY")
        )
        print(f"âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ: {model.model_id}")
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # åˆ›å»ºç¤ºä¾‹æ¶ˆæ¯
    messages = create_example_messages()
    
    # æ˜¾ç¤ºæ¶ˆæ¯è¯¦æƒ…
    print_messages_detailed(messages)
    
    # éæµå¼è°ƒç”¨ç¤ºä¾‹
    success = non_streaming_example(model, messages)
    if not success:
        print("âš ï¸  éæµå¼è°ƒç”¨å¤±è´¥ï¼Œè·³è¿‡åç»­ç¤ºä¾‹")
        return
    
    # æµå¼è°ƒç”¨ç¤ºä¾‹
    success = streaming_example(model, messages)
    if not success:
        print("âš ï¸  æµå¼è°ƒç”¨å¤±è´¥")
        return
    
    # äº¤äº’å¼èŠå¤©ç¤ºä¾‹
    try:
        interactive_chat(model)
    except Exception as e:
        print(f"âŒ äº¤äº’å¼èŠå¤©å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    load_dotenv()
    
    main()