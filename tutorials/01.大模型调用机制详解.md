# 大模型调用机制详解

## 概述

本文档详细介绍了基于 OpenAI 兼容 API 的大模型调用机制，包括输入输出格式、消息角色定义、流式与非流式调用的区别，以及相关的代码实现。

示例代码：recipes/run_model.py

## 输入机制

### 消息格式

模型输入采用结构化的消息数组，每条消息包含以下属性：

*   **role**: 消息发送者角色
    *   `system`: 系统角色，用于设置AI的行为和回复风格
    *   `user`: 用户角色，包含用户的问题或指令
    *   `assistant`: AI助手角色，用于多轮对话中的历史回复
    *   `tool`: 工具调用角色，用于函数调用场景

*   **content**: 消息文本内容

*   **tool\_calls** (可选): 工具调用信息，包含函数名称和参数

### 消息构建示例

```python
messages = [
    ChatMessage(role=MessageRole.SYSTEM, content="系统提示词"),
    ChatMessage(role=MessageRole.USER, content="用户问题")
]
```

### 系统提示词 (System Prompt)

系统提示词用于定义AI的角色和行为模式，例如：

*   "你是一个有帮助的AI助手，请用中文回答用户的问题"
*   "你是一个历史专家，请用生动有趣的方式介绍历史古迹"

系统提示词对模型的输出风格和内容范围有重要影响。

## 输出机制

### 非流式响应

非流式调用会等待模型生成完整回复后一次性返回：

```json
{
  "role": "assistant",
  "content": "完整的回复内容",
  "token_usage": {
    "input_tokens": 34,
    "output_tokens": 68
  }
}
```

### 流式响应

流式调用实时返回生成的文本片段：

```python
for delta in model.generate_stream(messages=messages):
    if delta.content:
        print(delta.content, end="", flush=True)
    if delta.token_usage:
        # 实时显示token使用情况
```

### Token 使用统计

*   **输入Token**: 用户消息和系统提示词消耗的token数量
*   **输出Token**: 模型生成回复消耗的token数量
*   **总Token**: 输入输出token之和，影响API调用成本

## 代码实现详解

### 模型初始化

```python
model = OpenAIServerModel(
    model_id=os.getenv("DEEPSEEK_MODEL_ID"),  # 模型标识
    api_base=os.getenv("DEEPSEEK_URL"),       # API端点
    api_key=os.getenv("DEEPSEEK_API_KEY"),    # 认证密钥
    temperature=0.7,                          # 创造性参数(0-1)
    max_tokens=500                            # 最大输出token数
)
```

### 消息格式化输出

`print_messages_detailed()` 函数提供美观的消息预览：

    🔍 输入的Prompt详情:
    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    ⚙️ 消息 1 [SYSTEM]:
       📄 内容: 你是一个有帮助的AI助手...
       ────────────────────────────────────────
    👤 消息 2 [USER]:
       📄 内容: 请解释一下人工智能...
       ────────────────────────────────────────

### 错误处理

代码包含完整的异常处理机制：

```python
try:
    response = model.generate(messages=messages)
except Exception as e:
    print(f"❌ 调用过程中出现错误: {e}")
```

## 应用场景

### 1. 非流式调用

适合需要完整回复后再进行处理的场景，如：

*   内容分析和提取
*   批量处理多个问题
*   需要完整上下文的应用

### 2. 流式调用

适合需要实时反馈的场景，如：

*   聊天机器人对话
*   长文本生成时的进度显示
*   需要低延迟响应的应用

## 最佳实践

1.  **系统提示词优化**: 明确指定AI的角色和回答风格
2.  **Token管理**: 监控token使用量以控制成本
3.  **错误处理**: 实现完整的异常处理机制
4.  **流式响应**: 对于用户体验要求高的应用使用流式调用
5.  **消息格式化**: 使用结构化消息提高可读性和调试效率

## 总结

该模型调用机制提供了灵活、高效的AI交互方式，通过结构化的消息输入和多种输出模式，能够满足不同场景下的需求。合理的系统提示词设计和token管理是优化模型表现和控制成本的关键因素。
